{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
      "metadata": {
        "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5"
      },
      "source": [
        "AI Technical Explainer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b890c58e-e17e-49a4-9955-7659a3d15428",
      "metadata": {
        "id": "b890c58e-e17e-49a4-9955-7659a3d15428"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31f949e0-ee72-48b1-824d-6dd01bc07228",
      "metadata": {
        "id": "31f949e0-ee72-48b1-824d-6dd01bc07228"
      },
      "outputs": [],
      "source": [
        "\n",
        "MODEL = \"gpt-4o-mini\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "253bb3d3-97f5-4ffa-9820-75942d42f92e",
      "metadata": {
        "id": "253bb3d3-97f5-4ffa-9820-75942d42f92e",
        "outputId": "c98c986d-e98e-4e46-c08f-a88861f5fd01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "api key exists and begins with sk-proj-\n"
          ]
        }
      ],
      "source": [
        "# set up environment\n",
        "load_dotenv(override=True)\n",
        "api_key = os.getenv(\"********\")\n",
        "if api_key:\n",
        "    print(f\"api key exists and begins with {api_key[:8]}\")\n",
        "else:\n",
        "    print(\"api key is not set\")\n",
        "\n",
        "openai = OpenAI()\n",
        "EXPLAIN_MODEL = MODEL\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b17cb53d-782f-439b-a991-804584802241",
      "metadata": {
        "id": "b17cb53d-782f-439b-a991-804584802241"
      },
      "outputs": [],
      "source": [
        "# ---- System prompt ----\n",
        "explain_system_prompt = (\n",
        "    \"You are a concise technical explainer. Given a technical question or code snippet, \"\n",
        "    \"produce a clear, accurate explanation with the following structure:\\n\"\n",
        "    \"1) What it is/does\\n\"\n",
        "    \"2) How it works (step-by-step)\\n\"\n",
        "    \"3) Why itâ€™s written that way (tradeoffs, idioms)\\n\"\n",
        "    \"4) Pitfalls & edge cases\\n\"\n",
        "    \"5) Tiny example(s) if helpful\\n\"\n",
        "    \"Keep it correct, brief, and formatted in Markdown.\"\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b7099db-9ecf-47ac-991a-c4b0700b0469",
      "metadata": {
        "id": "8b7099db-9ecf-47ac-991a-c4b0700b0469"
      },
      "outputs": [],
      "source": [
        "def get_explainer_user_prompt(question: str, context: str = \"\") -> str:\n",
        "    q = (question or \"\").strip()\n",
        "    ctx = (context or \"\").strip()\n",
        "    user_prompt = \"You are looking at a technical question.\\n\"\n",
        "    user_prompt += \"Explain it for a practitioner with clear headings.\\n\\n\"\n",
        "    user_prompt += \"Question:\\n\" + q + \"\\n\"\n",
        "    if ctx:\n",
        "        user_prompt += \"\\nAdditional context (if any):\\n\" + ctx + \"\\n\"\n",
        "    return user_prompt[:8000]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5198e76b-c13f-4e60-b88c-e86fd474c559",
      "metadata": {
        "id": "5198e76b-c13f-4e60-b88c-e86fd474c559"
      },
      "outputs": [],
      "source": [
        "def _coerce_messages_to_q_and_ctx(messages):\n",
        "    \"\"\"\n",
        "    Accepts either:\n",
        "      - list[{'role': ..., 'content': ...}, ...]  (gr.ChatInterface type='messages')\n",
        "      - str                                     (gr.ChatInterface type='text' or older versions)\n",
        "    Returns: (question_str, context_str)\n",
        "    \"\"\"\n",
        "    # Case 1: list of dicts (expected for type='messages')\n",
        "    if isinstance(messages, list):\n",
        "        # Some builds may pass plain strings inside; be defensive\n",
        "        cleaned = []\n",
        "        for m in messages:\n",
        "            if isinstance(m, dict):\n",
        "                cleaned.append(m)\n",
        "            elif isinstance(m, str):\n",
        "                cleaned.append({\"role\": \"user\", \"content\": m})\n",
        "            else:\n",
        "                cleaned.append({\"role\": \"user\", \"content\": str(m)})\n",
        "\n",
        "        user_msgs = [m for m in cleaned if m.get(\"role\") == \"user\" and isinstance(m.get(\"content\"), str)]\n",
        "        question = user_msgs[-1][\"content\"] if user_msgs else \"\"\n",
        "\n",
        "        context_parts = []\n",
        "        # Everything before last user message as context\n",
        "        for m in cleaned[:-1]:\n",
        "            c = m.get(\"content\", \"\")\n",
        "            if isinstance(c, str) and c.strip():\n",
        "                context_parts.append(c)\n",
        "        context = \"\\n\\n\".join(context_parts)\n",
        "        return question, context\n",
        "\n",
        "    # Case 2: plain string (type='text' or some frontends)\n",
        "    if isinstance(messages, str):\n",
        "        return messages, \"\"  # question is the whole string, no prior context\n",
        "\n",
        "    # Fallback: unknown shape\n",
        "    return str(messages), \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc976676-7e3d-424a-860d-07cdf7a7a36e",
      "metadata": {
        "id": "bc976676-7e3d-424a-860d-07cdf7a7a36e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def stream_explanation(messages, state=None, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Streaming function compatible with multiple Gradio versions.\n",
        "    Accepts either list[dict] or str and yields partial markdown.\n",
        "    \"\"\"\n",
        "    question, context = _coerce_messages_to_q_and_ctx(messages)\n",
        "\n",
        "    # ðŸ‘‹ Custom greeting if user just says \"hi\" or similar\n",
        "    if question.strip().lower() in {\"hi\", \"hello\", \"hey\"}:\n",
        "        yield \"I am a technical explainer â€” ask any technical question to get your answer.\"\n",
        "        return\n",
        "\n",
        "    stream = openai.chat.completions.create(\n",
        "        model=EXPLAIN_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": explain_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": get_explainer_user_prompt(question, context)},\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0.2,\n",
        "    )\n",
        "\n",
        "    partial = \"\"\n",
        "    for chunk in stream:\n",
        "        delta = chunk.choices[0].delta.content or \"\"\n",
        "        if not delta:\n",
        "            continue\n",
        "        partial += delta\n",
        "        # clean occasional fences\n",
        "        cleaned = partial.replace(\"```markdown\", \"\").replace(\"```\", \"\")\n",
        "        yield cleaned\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e2c39f7-e5dc-4b52-ace8-04e291b87935",
      "metadata": {
        "id": "6e2c39f7-e5dc-4b52-ace8-04e291b87935",
        "outputId": "7182f9cb-1075-4151-b277-09ceda49a309"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7865\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# -------- Gradio UI (streaming) --------\n",
        "demo = gr.ChatInterface(\n",
        "    fn=stream_explanation,          # generator for streaming\n",
        "    type=\"messages\",                # history as list of dicts (we also handle str fallback)\n",
        "    title=\"Your AI Technical Explainer\",\n",
        "    description=\"Ask me any technical question.\",\n",
        "    examples=[\n",
        "        [{\"role\": \"user\", \"content\": \"Explain Python's list comprehensions vs map/filter.\"}],\n",
        "        [{\"role\": \"user\", \"content\": \"What does this regex do? ^(?!.*password).*$\"}],\n",
        "        [{\"role\": \"user\", \"content\": \"Give me the gist of Python asyncio.\"}],\n",
        "    ],\n",
        "    cache_examples=False,\n",
        "    submit_btn=\"Explain â–¶\",\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}